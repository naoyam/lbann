trainer {
  block_size: 256
  procs_per_trainer: 0
  num_parallel_readers: 0
}
model {
  data_layout: "data_parallel"
  mini_batch_size: 11
  num_epochs: 0

  ###################################################
  # Objective function and metrics
  ###################################################

  objective_function {
    layer_term { layer: "l2" }
  }
  metric {
    layer_metric {
      layer: "l2"
      name: "L2 norm"
    }
  }

  ###################################################
  # Callbacks
  ###################################################

  callback { print {} }
  callback { timer {} }
  callback {
    check_metric {
      metric: "L2 norm" # Expected value: 5.25
      lower_bound: 5.249
      upper_bound: 5.251
      error_on_failure: true
      execution_modes: "test"
    }
  }
  callback {
    check_gradients {
      execution_modes: "test"
      verbose: false
      error_on_failure: true
    }
  }

  ###################################################
  # Layers
  ###################################################

  layer {
    name: "data"
    data_layout: "data_parallel"
    input {}
  }

  # Input data
  layer {
    name: "x"
    weights_layer {
      dims: "5"
    }
    data_layout: "model_parallel"
    weights: "x_vals"
  }
  weights {
    name: "x_vals"
    initializer {
      value_initializer {
        values: "-1.5 -0.25 0.25 0.5 1"
      }
    }
  }

  # Variations of ReLU layer
  layer {
    parents: "x"
    name: "relu_model_parallel"
    relu {}
    data_layout: "model_parallel"
  }
  layer {
    parents: "x"
    name: "relu_data_parallel"
    relu {}
    data_layout: "data_parallel"
  }

  # Combine into objective function
  layer {
    parents: "relu_model_parallel relu_data_parallel"
    name: "sum"
    sum {}
  }
  layer {
    parents: "sum"
    name: "l2"
    l2_norm2 {}
  }

}
