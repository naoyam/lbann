trainer {
  num_parallel_readers: 1
}
model {
  mini_batch_size: 1024
  num_epochs: 4

  ##############################################
  # Objective function
  ##############################################

  objective_function {
    layer_term { layer: "mean_squared_error" }
    layer_term {
      layer: "mean_absolute_error"
      scale_factor: 0.01
    }
    l2_weight_regularization {
      scale_factor: 0.0005
    }
  }

  ##############################################
  # Callbacks
  ##############################################

  callback { print {} }
  callback { timer {} }

  ##############################################
  # Layers
  ##############################################

  ######################
  # Data
  ######################

  layer {
    name: "input"
    children: "data label"
    data_layout: "data_parallel"
    input {}
  }
  layer {
    parents: "input"
    name: "data"
    data_layout: "data_parallel"
    reshape {
      dims: "1 -1" # Reshape to 1xX tensor
    }
  }
  layer {
    parents: "input"
    name: "label"
    data_layout: "data_parallel"
    dummy {}
  }

  ######################
  # Encoder
  ######################

  # encode1
  layer {
    parents: "data"
    name: "encode1_conv"
    data_layout: "data_parallel"
    convolution {
      num_dims: 1
      num_output_channels: 1024
      conv_dims: "240"
      conv_pads: "0"
      conv_strides: "240"
      has_bias: true
      has_vectors: true
    }
  }
  layer {
    parents: "encode1_conv"
    name: "encode1_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "encode1_bn"
    name: "encode1"
    data_layout: "data_parallel"
    relu {}
  }

  # encode2
  layer {
    parents: "encode1"
    name: "encode2_fc"
    data_layout: "data_parallel"
    fully_connected {
      num_neurons: 12
      has_bias: true
    }
  }
  layer {
    parents: "encode2_fc"
    name: "encode2_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "encode2_bn"
    name: "encode2"
    data_layout: "data_parallel"
    relu {}
  }

  ######################
  # Decoder
  ######################

  # decode2
  weights {
    name: "decode2_fc_matrix"
    initializer {
      glorot_uniform_initializer {}
    }
  }
  layer {
    parents: "encode2"
    name: "decode2_fc"
    weights: "decode2_fc_matrix"
    hint_layer: "encode1"
    data_layout: "data_parallel"
    fully_connected {
      has_bias: true
    }
  }
  layer {
    parents: "decode2_fc"
    name: "decode2_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "decode2_bn"
    name: "decode2"
    data_layout: "data_parallel"
    relu {}
  }

  # decode1
  layer {
    parents: "decode2"
    name: "decode1_deconv"
    data_layout: "data_parallel"
    deconvolution {
      num_dims: 1
      num_output_channels: 1
      conv_dims: "240"
      conv_pads: "0"
      conv_strides: "240"
      has_bias: true
      has_vectors: true
    }
  }
  layer {
    parents: "decode1_deconv"
    name: "decode1_bn"
    data_layout: "data_parallel"
    batch_normalization {
      decay: 0.9
      scale_init: 1.0
      bias_init: 0.0
      epsilon: 1e-5
    }
  }
  layer {
    parents: "decode1_bn"
    name: "decode1"
    data_layout: "data_parallel"
    elu {
    }
  }

  ######################
  # Reconstruction
  ######################

  layer {
    parents: "decode1 data"
    name: "mean_squared_error"
    data_layout: "data_parallel"
    mean_squared_error {}
  }
  layer {
    parents: "decode1 data"
    name: "mean_absolute_error"
    data_layout: "data_parallel"
    mean_absolute_error {}
  }

}
