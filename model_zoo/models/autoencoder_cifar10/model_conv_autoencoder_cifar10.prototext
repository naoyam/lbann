trainer {
  num_parallel_readers: 1
}
model {
  data_layout: "data_parallel"
  mini_batch_size: 128
  num_epochs: 10
  disable_cuda: true

  ###################################################
  # Objective function
  ###################################################

  objective_function {
    layer_term { layer: "mean_squared_error" }
    l2_weight_regularization {
      scale_factor: 0.0005
    }
  }

  ###################################################
  # Metrics
  ###################################################

  metric {
    layer_metric {
      name: "mean squared error"
      layer: "mean_squared_error"
    }
  }

  ###################################################
  # Callbacks
  ###################################################
  callback {
    print {
      interval: 1
    }
  }
  callback {
    timer {
    }
  }

  ###################################################
  # start of layers
  ###################################################

  #######
  # INPUT
  #######
  layer {
    name: "data"
    children: "image dummy"
    data_layout: "data_parallel"
    input {}
  }
  layer {
    parents: "data"
    name: "image"
    data_layout: "data_parallel"
    split {}
  }
  layer {
    parents: "data"
    name: "dummy"
    data_layout: "data_parallel"
    dummy {}
  }

  #############
  # CONVOLUTION 1
  #############
  layer {
    parents: "image"
    name: "conv1"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels: 16
      conv_dims: "3 3"
      conv_pads: "0 0"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  ######
  # RELU 1
  ######
  layer {
    parents: "conv1"
    name: "relu1"
    data_layout: "data_parallel"
    relu {
    }
  }

  #########
  # POOLING 1
  #########
  layer {
    parents: "relu1"
    name: "pool1"
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "2 2"
      pool_pads: "0 0"
      pool_strides: "1 1"
      pool_mode: "max"
      has_vectors: true
    }
  }

  #############
  # CONVOLUTION 2
  #############
  layer {
    parents: "pool1"
    name: "conv2"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels: 8
      conv_dims: "3 3"
      conv_pads: "0 0"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  ######
  # RELU 2
  ######
  layer {
    parents: "conv2"
    name: "relu2"
    data_layout: "data_parallel"
    relu {
    }
  }

  #########
  # POOLING 2
  #########
  layer {
    parents: "relu2"
    name: "pool2"
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "2 2"
      pool_pads: "0 0"
      pool_strides: "1 1"
      pool_mode: "max"
      has_vectors: true
    }
  }

  #############
  # CONVOLUTION 3
  #############
  layer {
    parents: "pool2"
    name: "conv3"
    data_layout: "data_parallel"
    convolution {
      num_dims: 2
      num_output_channels: 8
      conv_dims: "3 3"
      conv_pads: "0 0"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  ######
  # RELU 3
  ######
  layer {
    parents: "conv3"
    name: "relu3"
    data_layout: "data_parallel"
    relu {
    }
  }

  #########
  # POOLING 3
  #########
  layer {
    parents: "relu3"
    name: "pool3"
    data_layout: "data_parallel"
    pooling {
      num_dims: 2
      pool_dims: "2 2"
      pool_pads: "0 0"
      pool_strides: "1 1"
      pool_mode: "max"
      has_vectors: true
    }
  }

 #########
  # UNPOOLING 3
  #########
  layer {
    parents: "pool3"
    name: "unpool3"
    data_layout: "data_parallel"
    unpooling {
      num_dims: 2
      pooling_layer: "pool3"
    }
  }

  #############
  # DECONVOLUTION 3
  #############
  layer {
    parents: "unpool3"
    name: "deconv3"
    data_layout: "data_parallel"
    deconvolution {
      num_dims: 2
      num_output_channels: 8
      conv_dims: "3 3"
      conv_pads: "0 0"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  ######
  # RELU 4
  ######
  layer {
    parents: "deconv3"
    name: "relu4"
    data_layout: "data_parallel"
    relu {
    }
  }

  #########
  # UNPOOLING 2
  #########
  layer {
    parents: "relu4"
    name: "unpool2"
    data_layout: "data_parallel"
    unpooling {
      num_dims: 2
      pooling_layer: "pool2"
    }
  }

  #############
  # DECONVOLUTION 2
  #############
  layer {
    parents: "unpool2"
    name: "deconv2"
    data_layout: "data_parallel"
    deconvolution {
      num_dims: 2
      num_output_channels: 16
      conv_dims: "3 3"
      conv_pads: "0 0"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  ######
  # RELU 5
  ######
  layer {
    parents: "deconv2"
    name: "relu5"
    data_layout: "data_parallel"
    relu {
    }
  }

  #########
  # UNPOOLING 1
  #########
  layer {
    parents: "relu5"
    name: "unpool1"
    data_layout: "data_parallel"
    unpooling {
      num_dims: 2
      pooling_layer: "pool1"
    }
  }

  #############
  # DECONVOLUTION 1
  #############
  layer {
    parents: "unpool1"
    name: "deconv1"
    data_layout: "data_parallel"
    deconvolution {
      num_dims: 2
      num_output_channels: 3
      conv_dims: "3 3"
      conv_pads: "0 0"
      conv_strides: "1 1"
      has_bias: true
      has_vectors: true
    }
  }

  ######
  # RELU 6
  ######
  layer {
    parents: "deconv1"
    name: "relu6"
    data_layout: "data_parallel"
    relu {
    }
  }


  #################
  # FULLY_CONNECTED decode1
  #################
  layer {
    parents: "relu6"
    name: "decode1"
    data_layout: "data_parallel"
    hint_layer: "image"
    fully_connected {
      num_neurons: 784
      has_bias: true
    }
  }

  #######
  # SIGMOID sigmoid
  #######
  layer {
    parents: "decode1"
    name: "reconstruction"
    data_layout: "data_parallel"
    sigmoid {
    }
  }


  #################
  # RECONSTRUCTION
  #################
  layer {
    parents: "reconstruction image"
    name: "mean_squared_error"
    data_layout: "data_parallel"
    mean_squared_error {}
  }

  ###################################################
  # end of layers
  ###################################################
}
